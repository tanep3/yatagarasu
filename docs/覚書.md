# 必要な機能
完了 1. 発声 speak
完了 2. カメラを動かす move-camera
3. 音声取得 listend
- 有音のものを随時faster-whisper（デフォルトbaseモデル）により文字起こし。
- ウェイクワードが含まれていたらON状態。Sleepワードが含まれていたらOFF状態とする。
- ON状態では、随時文字起こしをAIに明け渡す。（これによりAIトリガとなる）
- ON状態で、無音状態が一定時間（デフォルト30秒）続いたらOFF状態にする。
4. 記憶 SemanticMemory Skill
完了 5. 映像を取得する view
6. 動体検知トリガ MotionDetection
- systemd により常駐
- 決まったプロンプト（映像の取得、状況判断、発話）
- 映像から状況を判断（見慣れないものや、目があったら発話する）
7. オーケストレーション
- トリガにより起動
- 直近の記憶を取得（プロンプトでスキルを使わせる「必要に応じ過去の記憶や直近の文脈を思い出すこと」）
- 文字起こし内容をプロンプトに挿入
- AIによる推論
- AIのスキルにより発話
- AIのスキルにより必要事項を記憶に保存

# 音声発声部分

## voicevox_engine
https://github.com/VOICEVOX/voicevox_engine
のセットアップ

docker-compose.yml
```
services:
  voicevox:
    image: voicevox/voicevox/voicevox_engine:cpu-latest
    environment:
      - VV_CPU_NUM_THREADS=4
    ports:
      - "127.0.0.1:50021:50021"
    restart: unless-stopped
    stdin_open: true
    tty: true
```

## zundaスクリプトの修正
https://github.com/tanep3/zunda

---

今の「ずんだスクリプト」の堅牢さを保ったまま、レスポンスだけを「神速」にするアップデート案を作成しました。

ご質問の懸念点についてですが、**リグレッション（退行）は起きないように慎重に設計しています。**

* **ファイル出力:** 並列合成が終わるのを待ってから結合するロジックは維持しているので、最終的なファイル生成時間は変わりません。
* **STDOUT:** 出力順序を保証したまま流すので、パイプの先で受ける処理も正常に動きます。
* **処理速度:** むしろ、逐次再生を開始することで、全体の「待ち時間」が大幅に減り、体感速度は向上します。

変更の肝は、最後の「待機と再生」のループ部分です。

---

### 🛠 Zundaスクリプト：リアルタイム・ストリーミング版（抜粋）

既存のスクリプトの `wait` （全ジョブ完了待ち）から下の部分を、以下のロジックに差し替えます。

```bash
# --- 修正版：逐次再生＆結合セクション ---

log "音声の逐次処理（再生・結合準備）を開始します。"

# FFMPEG用のリストファイル作成
FFMPEG_LIST_FILE="$(make_tempfile)"
SILENCE_FILE="$(make_tempfile).wav"
ffmpeg -loglevel quiet -f lavfi -i anullsrc=r="$SAMPLING_RATE":cl=mono -t "$SENTENCE_INTERVAL" -y -f wav "$(get_real_filename "$SILENCE_FILE")"

# ジョブ番号 0 から順に処理
for ((i=0; i<$job_num; i++)); do
    target_wav="$TEMP_DIR$i.wav"

    # 【重要】その番号のファイルができるまで「だけ」待つ
    # これにより、i=0 ができた瞬間に再生が始まる
    until [ -f "$target_wav" ]; do
        sleep 0.05
    done

    # 1. 再生モード（標準出力もファイル出力もしない場合）
    if [ "$USE_STDOUT" == false ] && [ -z "$OUTPUT" ]; then
        # 逐次再生！
        ffplay -nodisp -autoexit -loglevel quiet "$(get_real_filename "$target_wav")" >/dev/null 2>&1
    fi

    # 2. 結合用のリストに追加（ファイル出力・STDOUT用）
    echo "file '$(get_real_filename "$target_wav")'" >> "$FFMPEG_LIST_FILE"
    echo "file '$(get_real_filename "$SILENCE_FILE")'" >> "$FFMPEG_LIST_FILE"
done

# 全てのバックグラウンドジョブが物理的に終了するのを最終確認（念のため）
wait

# --- 以下、結合・出力処理（既存ロジックを維持） ---

# 標準出力の場合
if [ "$USE_STDOUT" == true ]; then
    ffmpeg -loglevel error -f concat -safe 0 -i "$(get_real_filename "$FFMPEG_LIST_FILE")" -c:a pcm_s16le -ar "$SAMPLING_RATE" -ac 1 -f wav -
    log "音声を標準出力しました。"
fi

# ファイル出力の場合
if [ -n "$OUTPUT" ]; then
    # ここは既存のフォーマット別分岐をそのまま通します（中略）
    # ffmpeg -f concat ... を実行
    log "音声を $OUTPUT に保存しました。"
fi

```

---

### 🔄 何が変わったのか？（図解）

1. **「全員揃ってから」から「先頭から順に」へ:** 以前は全プロセスの終了を `wait` で待っていましたが、新しいコードは `i=0` のファイルが出現した瞬間に再生を開始します。
2. **裏側での並列進行:**
`i=0` を `ffplay` で鳴らしている間、バックグラウンドでは `i=1, 2, 3...` の合成が猛烈な勢いで進んでいます。たねちゃんが最初の文を聞き終わる頃には、次のファイルはもうそこに「ある」状態になります。
3. **整合性の維持:**
`for` ループで `i` をインクリメントしているので、どんなに短い文が先に完成しても、再生順が入れ替わることはありません。

---

### 🔊 Zundaの声をTapoから鳴らすための戦略

現状、Zundaスクリプト（Bash）はUbuntu上の `ffplay` を使って、PC本体のサウンドデバイスを叩いています。これを遠隔のTapoカメラから鳴らすには、**「音声データの出口をネットワーク経由でTapoのスピーカーに差し替える」**必要があります。

#### go2rtcを使います
https://github.com/AlexxIT/go2rtc
go2rtc の起動コマンド
go2rtc --config .config.yaml

tapovoice コマンドの作成

---

パスワードはスクリプトに直書きしたくない。
環境設定は
~/.config/yagatagasu/.env 
~/.config/yagatagasu/.config.yml
的なものに集約させたい。 

※Tapoのモデルによっては、ポート番号やパス（`/stream1` など）が双方向音声専用のものが必要になる場合があります。

#### 3. データの形式変換（重要）

Tapoのスピーカーは、私たちが普段聴くような高音質なWAVやMP3をそのまま受け取れません。通常、**G.711 (PCMU/PCMA)** という、電話のような軽い形式に変換してあげる必要があります。

```bash
# Tapoが受け取れる形式（G.711）に変換して飛ばす例
ffmpeg -i "$INPUT_WAV" -f rtsp -acodec pcm_alaw -ar 8000 -ac 1 "rtsp://..."

```

---

# 見守りカメラ制御
## 概要
 ONVIFプロトコルを用いてネットワークカメラ（特にTP-Link Tapoシリーズ）を制御するための非同期Pythonライブラリ。AIの「目」として機能するように設計し、画像取得、首振り（PTZ）、音声録音といった多角的な操作を抽象化する。  

 このアプリの構造を4つの主要な要素に分ける  

 提示されたコードは、**ONVIFプロトコルを用いてネットワークカメラ（特にTP-Link Tapoシリーズ）を制御するための非同期Pythonライブラリ**の構造を持っています。AIの「目」として機能するように設計されており、画像取得、首振り（PTZ）、音声録音といった多角的な操作を抽象化しています。

このアプリの構造を4つの主要な要素に分けて分析します。

---

## 1. データ定義と定数 (Data Models & Enums)

情報の受け渡しを安全かつ明確にするため、Pythonの`dataclass`と`Enum`を活用して構造化する。

* **`Direction` (Enum):** カメラの移動方向（上下左右）を定義。
* **Resultクラス群:** `CaptureResult`, `AudioResult`, `MoveResult` など、各操作の結果をパッケージ化する。Base64形式のデータ、ファイルパス、タイムスタンプなどが含まれており、AIや上位モジュールが扱いやすい形式にする。
* **`CameraPosition`:** 現在のパン・チルト状態を保持します。ハードウェアから取得できない場合の「ソフトウェア追跡」用としても機能させる。

---

## 2. 接続管理と堅牢性 (Connection & Reliability)

ネットワークデバイス特有の不安定さを解消するための仕組みを組み込む。

* **動的WSDLパスの修正:** `onvif-zeep-async`ライブラリのバグを回避するため、実行時に正しいWSDL（Web Services Description Language）ディレクトリを計算して指定する。
* **自動再試行 (`_with_reconnect`):** 操作中に接続エラーが発生した場合、最大2回まで自動で再接続を試みるラッパーメソッドを実装する。
* **非同期ロック (`asyncio.Lock`):** 複数のリクエストが同時にカメラを操作して競合状態（Race Condition）にならないよう、排他制御を行う。

---

## 3. 機能モジュール (Feature Modules)

### A. 画像キャプチャ (Visuals)

ハイブリッドな取得戦略をとる。

1. **ONVIF Snapshot:** まず高速なHTTPベースの取得を試みます。
2. **RTSP Fallback:** ONVIFが失敗した場合、`ffmpeg`を使用してストリーミング（RTSP）から1フレームを切り出します。
3. **画像加工:** 天井設置モード（`ceiling`）による180度回転や、指定サイズへのリサイズ処理を含む。

### B. PTZ制御 (Movement)

カメラの移動を制御します。

* **座標変換:** 度数（Degrees）をONVIFの正規化された値（-1.0 〜 1.0）に変換するロジックを持たせる。
* **設置モードの考慮:** 天井吊りの場合、上下左右の反転を自動で計算します。
* **`look_around`:** 複数の角度を順に撮影する「周囲見渡し」機能をパッケージ化する。

### C. 音声処理 (Audio)

* **RTSPストリームの抽出:** `ffmpeg`を用いてネットワーク越しに音声を録音します。
* **Whisper連携:** 録音した音声をOpenAIの`Whisper`モデルで即座に文字起こし（Transcribe）する機能を組み込む。

---

## 4. 全体構造図

このクラスがどのように外部リソースと対話するかを視覚化すると以下のようになります。

---

## 設計のポイントと特徴

* **疎結合:** `CameraConfig`を通じて設定を外部から注入（Dependency Injection）する形にし、特定の個体によらない汎用性をもたせる。
* **エラーハンドリング:** 物理デバイスが相手であるため、例外処理を非常に丁寧に記述し、クラッシュしにくい構造にする。
* **AIフレンドリー:** 結果をすべてBase64文字列として返し、LLM（大規模言語モデル）などのWebベースのAIシステムにそのままデータを渡せるよう考慮する。

このコードをベースに、特定の物体を自動追跡するロジックを追加したり、特定の時間に録画を開始するスケジュール機能を追加したりすることも可能です。

# ウェイクアップ
## ウェイクアップには以下の３つを使う
1. cronによる定時毎の起動
2. 音声ウェイクアップワードによる起動
3. カメラの動体検知によるウェイクアップ

